{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595216627809",
   "display_name": "Python 3.6.10 64-bit ('imbd2020': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from imbd.trainers import ModelTrainer\n",
    "from imbd.data import DataLoader\n",
    "from imbd.preprocessors import DataPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader()\n",
    "prepro = DataPreprocessor()\n",
    "\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.1))\n",
    "    model.add(tf.keras.layers.Dense(20))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_func_model():\n",
    "    inputs = tf.keras.Input(shape=(293,))\n",
    "    x = tf.keras.layers.Dense(64)(inputs)\n",
    "    outputs = tf.keras.layers.Dense(20)(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    param_grid = {\n",
    "        \"prepro__variance_selector__threshold\": [0.0],\n",
    "        # \"model__estimator__n_estimators\": [1000],\n",
    "        # \"model__estimator__max_depth\": [5, 10],\n",
    "        # \"model__estimator__alpha\": [0, 0.1, 0.01],\n",
    "        # \"model__estimator__lambda\": [1, 0.5, 0.1],\n",
    "        # \"model__estimator__subsample\": [1, 0.5],\n",
    "        # \"model__estimator__gamma\": [0, 2, 10],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "loader = DataLoader()\n",
    "preprocessor = DataPreprocessor()\n",
    "df = loader.build()\n",
    "\n",
    "# get feature & label\n",
    "train_features = df.drop(loader.labels, axis=1)\n",
    "train_labels = df[loader.labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MultiOutputRegressor(XGBRegressor())\n",
    "# base_nn_model = KerasRegressor(build_fn=create_model, epochs=100)\n",
    "base_nn_model = KerasRegressor(build_fn=create_model, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "steps = [('prepro', preprocessor), ('model', base_nn_model)]\n",
    "pipe = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n[CV] prepro__variance_selector__threshold=0.0 ........................\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n292\nEpoch 1/100\nWARNING:tensorflow:Model was constructed with shape (None, 293) for input Tensor(\"input_1:0\", shape=(None, 293), dtype=float32), but it was called on an input with incompatible shape (None, 292).\n[CV] ......... prepro__variance_selector__threshold=0.0, total=   1.1s\n[CV] prepro__variance_selector__threshold=0.0 ........................\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s remaining:    0.0s\n289\nEpoch 1/100\nWARNING:tensorflow:Model was constructed with shape (None, 293) for input Tensor(\"input_2:0\", shape=(None, 293), dtype=float32), but it was called on an input with incompatible shape (None, 289).\n[CV] ......... prepro__variance_selector__threshold=0.0, total=   0.8s\n[CV] prepro__variance_selector__threshold=0.0 ........................\n293\nEpoch 1/100\n8/8 [==============================] - 0s 6ms/step - loss: 8218.0391\nEpoch 2/100\n8/8 [==============================] - 0s 5ms/step - loss: 1823.0842\nEpoch 3/100\n8/8 [==============================] - 0s 5ms/step - loss: 343.2648\nEpoch 4/100\n8/8 [==============================] - 0s 2ms/step - loss: 113.7823\nEpoch 5/100\n8/8 [==============================] - 0s 2ms/step - loss: 139.2717\nEpoch 6/100\n8/8 [==============================] - 0s 3ms/step - loss: 80.4604\nEpoch 7/100\n8/8 [==============================] - 0s 3ms/step - loss: 12.3296\nEpoch 8/100\n8/8 [==============================] - 0s 5ms/step - loss: 5.6293\nEpoch 9/100\n8/8 [==============================] - 0s 6ms/step - loss: 6.9731\nEpoch 10/100\n8/8 [==============================] - 0s 2ms/step - loss: 3.1624\nEpoch 11/100\n8/8 [==============================] - 0s 2ms/step - loss: 1.0915\nEpoch 12/100\n8/8 [==============================] - 0s 3ms/step - loss: 1.1152\nEpoch 13/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.9929\nEpoch 14/100\n8/8 [==============================] - 0s 10ms/step - loss: 0.7720\nEpoch 15/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.7592\nEpoch 16/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.7332\nEpoch 17/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.7044\nEpoch 18/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.7029\nEpoch 19/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.6947\nEpoch 20/100\n8/8 [==============================] - 0s 5ms/step - loss: 0.6843\nEpoch 21/100\n8/8 [==============================] - 0s 6ms/step - loss: 0.6750\nEpoch 22/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.6548\nEpoch 23/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.6482\nEpoch 24/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.6847\nEpoch 25/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.6467\nEpoch 26/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.6139\nEpoch 27/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.6111\nEpoch 28/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.5942\nEpoch 29/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.5911\nEpoch 30/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.5845\nEpoch 31/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.5736\nEpoch 32/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.5501\nEpoch 33/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.5501\nEpoch 34/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.5485\nEpoch 35/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.5532\nEpoch 36/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.5310\nEpoch 37/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.5190\nEpoch 38/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.5160\nEpoch 39/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.5092\nEpoch 40/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.5032\nEpoch 41/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.5063\nEpoch 42/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.4911\nEpoch 43/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.4959\nEpoch 44/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.4960\nEpoch 45/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.4850\nEpoch 46/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.4772\nEpoch 47/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.4851\nEpoch 48/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.4916\nEpoch 49/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.4797\nEpoch 50/100\n8/8 [==============================] - 0s 5ms/step - loss: 0.4604\nEpoch 51/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.4322\nEpoch 52/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.4181\nEpoch 53/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.4166\nEpoch 54/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.4166\nEpoch 55/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.4153\nEpoch 56/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.4007\nEpoch 57/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.4117\nEpoch 58/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.3938\nEpoch 59/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.3856\nEpoch 60/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.3854\nEpoch 61/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.3991\nEpoch 62/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.3872\nEpoch 63/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.3866\nEpoch 64/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.3769\nEpoch 65/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.3717\nEpoch 66/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.3582\nEpoch 67/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.3540\nEpoch 68/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.3483\nEpoch 69/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.3435\nEpoch 70/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.3368\nEpoch 71/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.3342\nEpoch 72/100\n8/8 [==============================] - 0s 7ms/step - loss: 0.3274\nEpoch 73/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.3263\nEpoch 74/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.3381\nEpoch 75/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.3410\nEpoch 76/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.3285\nEpoch 77/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.3221\nEpoch 78/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.3242\nEpoch 79/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.3291\nEpoch 80/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.3236\nEpoch 81/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.3114\nEpoch 82/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.3027\nEpoch 83/100\n8/8 [==============================] - 0s 5ms/step - loss: 0.3129\nEpoch 84/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.2933\nEpoch 85/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.2998\nEpoch 86/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.2971\nEpoch 87/100\n8/8 [==============================] - 0s 5ms/step - loss: 0.2883\nEpoch 88/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.2779\nEpoch 89/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.2996\nEpoch 90/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.2854\nEpoch 91/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.2738\nEpoch 92/100\n8/8 [==============================] - 0s 5ms/step - loss: 0.2700\nEpoch 93/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.2727\nEpoch 94/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.2668\nEpoch 95/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.2737\nEpoch 96/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.2804\nEpoch 97/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.2730\nEpoch 98/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.2685\nEpoch 99/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.2555\nEpoch 100/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.2570\n293\n[CV] ......... prepro__variance_selector__threshold=0.0, total=   6.6s\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    8.6s finished\n293\nEpoch 1/100\n11/11 [==============================] - 0s 11ms/step - loss: 6473.6719\nEpoch 2/100\n11/11 [==============================] - 0s 3ms/step - loss: 658.5433\nEpoch 3/100\n11/11 [==============================] - 0s 2ms/step - loss: 183.0905\nEpoch 4/100\n11/11 [==============================] - 0s 2ms/step - loss: 107.4692\nEpoch 5/100\n11/11 [==============================] - 0s 3ms/step - loss: 27.5888\nEpoch 6/100\n11/11 [==============================] - 0s 2ms/step - loss: 6.6681\nEpoch 7/100\n11/11 [==============================] - 0s 3ms/step - loss: 4.9384\nEpoch 8/100\n11/11 [==============================] - 0s 3ms/step - loss: 1.3150\nEpoch 9/100\n11/11 [==============================] - 0s 2ms/step - loss: 1.1223\nEpoch 10/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.8013\nEpoch 11/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.7347\nEpoch 12/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.7024\nEpoch 13/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.6799\nEpoch 14/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.6626\nEpoch 15/100\n11/11 [==============================] - 0s 4ms/step - loss: 0.6522\nEpoch 16/100\n11/11 [==============================] - 0s 6ms/step - loss: 0.6376\nEpoch 17/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.6213\nEpoch 18/100\n11/11 [==============================] - 0s 4ms/step - loss: 0.6050\nEpoch 19/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.5927\nEpoch 20/100\n11/11 [==============================] - 0s 2ms/step - loss: 0.5848\nEpoch 21/100\n11/11 [==============================] - 0s 4ms/step - loss: 0.5715\nEpoch 22/100\n11/11 [==============================] - 0s 4ms/step - loss: 0.5563\nEpoch 23/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.5473\nEpoch 24/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.5354\nEpoch 25/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.5299\nEpoch 26/100\n11/11 [==============================] - 0s 10ms/step - loss: 0.5252\nEpoch 27/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.5177\nEpoch 28/100\n11/11 [==============================] - 0s 4ms/step - loss: 0.4964\nEpoch 29/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.4871\nEpoch 30/100\n11/11 [==============================] - 0s 2ms/step - loss: 0.4780\nEpoch 31/100\n11/11 [==============================] - 0s 4ms/step - loss: 0.4673\nEpoch 32/100\n11/11 [==============================] - 0s 4ms/step - loss: 0.4555\nEpoch 33/100\n11/11 [==============================] - 0s 2ms/step - loss: 0.4489\nEpoch 34/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.4434\nEpoch 35/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.4346\nEpoch 36/100\n11/11 [==============================] - 0s 7ms/step - loss: 0.4270\nEpoch 37/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.4228\nEpoch 38/100\n11/11 [==============================] - 0s 2ms/step - loss: 0.4124\nEpoch 39/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.4061\nEpoch 40/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.3999\nEpoch 41/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.3953\nEpoch 42/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.3884\nEpoch 43/100\n11/11 [==============================] - 0s 2ms/step - loss: 0.3786\nEpoch 44/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.3745\nEpoch 45/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.3686\nEpoch 46/100\n11/11 [==============================] - 0s 2ms/step - loss: 0.3669\nEpoch 47/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.3556\nEpoch 48/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.3519\nEpoch 49/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.3449\nEpoch 50/100\n11/11 [==============================] - 0s 5ms/step - loss: 0.3378\nEpoch 51/100\n11/11 [==============================] - 0s 4ms/step - loss: 0.3353\nEpoch 52/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.3280\nEpoch 53/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.3254\nEpoch 54/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.3211\nEpoch 55/100\n11/11 [==============================] - 0s 2ms/step - loss: 0.3215\nEpoch 56/100\n11/11 [==============================] - 0s 4ms/step - loss: 0.3121\nEpoch 57/100\n11/11 [==============================] - 0s 2ms/step - loss: 0.3039\nEpoch 58/100\n11/11 [==============================] - 0s 2ms/step - loss: 0.3043\nEpoch 59/100\n11/11 [==============================] - 0s 2ms/step - loss: 0.2970\nEpoch 60/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.2891\nEpoch 61/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.2887\nEpoch 62/100\n11/11 [==============================] - 0s 2ms/step - loss: 0.2836\nEpoch 63/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.2828\nEpoch 64/100\n11/11 [==============================] - 0s 4ms/step - loss: 0.2763\nEpoch 65/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.2739\nEpoch 66/100\n11/11 [==============================] - 0s 2ms/step - loss: 0.2699\nEpoch 67/100\n11/11 [==============================] - 0s 4ms/step - loss: 0.2636\nEpoch 68/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.2618\nEpoch 69/100\n11/11 [==============================] - 0s 2ms/step - loss: 0.2572\nEpoch 70/100\n11/11 [==============================] - 0s 5ms/step - loss: 0.2563\nEpoch 71/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.2513\nEpoch 72/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.2475\nEpoch 73/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.2485\nEpoch 74/100\n11/11 [==============================] - 0s 7ms/step - loss: 0.2468\nEpoch 75/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.2471\nEpoch 76/100\n11/11 [==============================] - 0s 2ms/step - loss: 0.2399\nEpoch 77/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.2360\nEpoch 78/100\n11/11 [==============================] - 0s 2ms/step - loss: 0.2314\nEpoch 79/100\n11/11 [==============================] - 0s 2ms/step - loss: 0.2297\nEpoch 80/100\n11/11 [==============================] - 0s 4ms/step - loss: 0.2251\nEpoch 81/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.2223\nEpoch 82/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.2197\nEpoch 83/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.2171\nEpoch 84/100\n11/11 [==============================] - 0s 4ms/step - loss: 0.2180\nEpoch 85/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.2141\nEpoch 86/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.2090\nEpoch 87/100\n11/11 [==============================] - 0s 4ms/step - loss: 0.2071\nEpoch 88/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.2101\nEpoch 89/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.2130\nEpoch 90/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.2092\nEpoch 91/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.2028\nEpoch 92/100\n11/11 [==============================] - 0s 4ms/step - loss: 0.1962\nEpoch 93/100\n11/11 [==============================] - 0s 11ms/step - loss: 0.1989\nEpoch 94/100\n11/11 [==============================] - 0s 2ms/step - loss: 0.1916\nEpoch 95/100\n11/11 [==============================] - 0s 2ms/step - loss: 0.1890\nEpoch 96/100\n11/11 [==============================] - 0s 2ms/step - loss: 0.1922\nEpoch 97/100\n11/11 [==============================] - 0s 8ms/step - loss: 0.1906\nEpoch 98/100\n11/11 [==============================] - 0s 4ms/step - loss: 0.1938\nEpoch 99/100\n 1/11 [=>............................] - ETA: 0s - loss: 0.17611/11 [==============================] - 0s 2ms/step - loss: 0.1897\nEpoch 100/100\n11/11 [==============================] - 0s 3ms/step - loss: 0.1806\n"
    }
   ],
   "source": [
    "# training\n",
    "trainer = ModelTrainer(pipe=pipe, param_grid=param_grid, verbose=2)\n",
    "fitted = trainer.train(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "nan"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "fitted.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "293\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     Input_A6_024  Input_A3_016  Input_C_013  Input_A2_016  Input_A3_017  \\\n0        0.286742      0.486058     1.061660      0.062364      0.661478   \n1        0.202472      0.222659     0.102260      0.178699      0.120206   \n2        0.041830      0.286463     0.166358      0.269153      0.387779   \n3        0.501441      0.860935     0.812607      0.181665      0.489082   \n4        0.402066      0.288642     0.243648      0.388051      0.527425   \n..            ...           ...          ...           ...           ...   \n334      0.221643      0.137322     0.415535      0.287241      0.174811   \n335      1.014708      0.546206     0.264109      0.030659      0.210615   \n336      0.444172      0.271113     0.015033      0.111857      0.426393   \n337      0.107421      0.371932     0.629629      0.589626      0.270849   \n338      0.856475      0.288589     0.289064      0.103272      0.136431   \n\n     Input_C_050  Input_A6_001  Input_C_096  Input_A3_018  Input_A6_019  \\\n0       0.531679      0.151108     0.062715      0.057925      0.441707   \n1       0.522276      0.761442     0.454674      0.907476      0.048870   \n2       0.502470      0.187407     0.400345      0.201411      0.231370   \n3       0.548579      0.352739     0.648485      0.782033      1.138806   \n4       0.788948      0.041971     0.251888      0.427016      0.245842   \n..           ...           ...          ...           ...           ...   \n334     0.083198      0.832583     0.294103      0.264600      0.216215   \n335     0.074433      0.013914     0.897492      0.106254      0.381597   \n336     0.033244      0.360036     0.916466      0.668931      0.152731   \n337     0.044645      0.225328     0.062849      0.192133      0.298835   \n338     0.149399      0.090540     0.286858      0.723929      0.211244   \n\n     Input_A1_020  Input_A6_011  Input_A3_015  Input_C_046  Input_C_049  \\\n0        0.071889      0.433032      1.261627     0.584437     0.331451   \n1        0.241779      0.152193      0.044849     0.477749     0.685401   \n2        0.240200      0.516925      0.469276     0.313096     0.124536   \n3        0.520106      0.301844      0.122653     0.631043     0.505454   \n4        0.369337      0.073092      0.035672     0.345505     0.051996   \n..            ...           ...           ...          ...          ...   \n334      0.492516      0.315594      0.291219     0.578705     0.350224   \n335      0.503782      0.205373      0.082455     0.004882     0.329083   \n336      0.860954      0.209770      0.148713     0.292803     0.420948   \n337      0.769998      0.572180      0.596455     0.274310     0.671780   \n338      0.654191      0.526241      0.205893     0.147172     0.151404   \n\n     Input_A2_024  Input_C_058  Input_C_057  Input_A3_013  Input_A2_017  \n0        0.612257     0.387300     0.408022      0.545370      0.136589  \n1        0.162297     0.040892     0.161624      0.235434      0.111094  \n2        0.044421     0.885170     0.380450      0.174234      0.008393  \n3        0.620362     0.853913     0.628278      0.010393      0.039976  \n4        0.081450     0.009500     0.575735      0.445435      0.040300  \n..            ...          ...          ...           ...           ...  \n334      0.350926     0.506324     0.148171      0.046526      0.056868  \n335      1.168812     0.055876     0.174809      0.199637      0.125589  \n336      0.427907     0.056493     0.138351      0.597641      0.247192  \n337      0.624567     0.053178     0.225962      0.735758      0.392469  \n338      0.429274     0.305417     0.026174      0.054776      1.041088  \n\n[339 rows x 20 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Input_A6_024</th>\n      <th>Input_A3_016</th>\n      <th>Input_C_013</th>\n      <th>Input_A2_016</th>\n      <th>Input_A3_017</th>\n      <th>Input_C_050</th>\n      <th>Input_A6_001</th>\n      <th>Input_C_096</th>\n      <th>Input_A3_018</th>\n      <th>Input_A6_019</th>\n      <th>Input_A1_020</th>\n      <th>Input_A6_011</th>\n      <th>Input_A3_015</th>\n      <th>Input_C_046</th>\n      <th>Input_C_049</th>\n      <th>Input_A2_024</th>\n      <th>Input_C_058</th>\n      <th>Input_C_057</th>\n      <th>Input_A3_013</th>\n      <th>Input_A2_017</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.286742</td>\n      <td>0.486058</td>\n      <td>1.061660</td>\n      <td>0.062364</td>\n      <td>0.661478</td>\n      <td>0.531679</td>\n      <td>0.151108</td>\n      <td>0.062715</td>\n      <td>0.057925</td>\n      <td>0.441707</td>\n      <td>0.071889</td>\n      <td>0.433032</td>\n      <td>1.261627</td>\n      <td>0.584437</td>\n      <td>0.331451</td>\n      <td>0.612257</td>\n      <td>0.387300</td>\n      <td>0.408022</td>\n      <td>0.545370</td>\n      <td>0.136589</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.202472</td>\n      <td>0.222659</td>\n      <td>0.102260</td>\n      <td>0.178699</td>\n      <td>0.120206</td>\n      <td>0.522276</td>\n      <td>0.761442</td>\n      <td>0.454674</td>\n      <td>0.907476</td>\n      <td>0.048870</td>\n      <td>0.241779</td>\n      <td>0.152193</td>\n      <td>0.044849</td>\n      <td>0.477749</td>\n      <td>0.685401</td>\n      <td>0.162297</td>\n      <td>0.040892</td>\n      <td>0.161624</td>\n      <td>0.235434</td>\n      <td>0.111094</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.041830</td>\n      <td>0.286463</td>\n      <td>0.166358</td>\n      <td>0.269153</td>\n      <td>0.387779</td>\n      <td>0.502470</td>\n      <td>0.187407</td>\n      <td>0.400345</td>\n      <td>0.201411</td>\n      <td>0.231370</td>\n      <td>0.240200</td>\n      <td>0.516925</td>\n      <td>0.469276</td>\n      <td>0.313096</td>\n      <td>0.124536</td>\n      <td>0.044421</td>\n      <td>0.885170</td>\n      <td>0.380450</td>\n      <td>0.174234</td>\n      <td>0.008393</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.501441</td>\n      <td>0.860935</td>\n      <td>0.812607</td>\n      <td>0.181665</td>\n      <td>0.489082</td>\n      <td>0.548579</td>\n      <td>0.352739</td>\n      <td>0.648485</td>\n      <td>0.782033</td>\n      <td>1.138806</td>\n      <td>0.520106</td>\n      <td>0.301844</td>\n      <td>0.122653</td>\n      <td>0.631043</td>\n      <td>0.505454</td>\n      <td>0.620362</td>\n      <td>0.853913</td>\n      <td>0.628278</td>\n      <td>0.010393</td>\n      <td>0.039976</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.402066</td>\n      <td>0.288642</td>\n      <td>0.243648</td>\n      <td>0.388051</td>\n      <td>0.527425</td>\n      <td>0.788948</td>\n      <td>0.041971</td>\n      <td>0.251888</td>\n      <td>0.427016</td>\n      <td>0.245842</td>\n      <td>0.369337</td>\n      <td>0.073092</td>\n      <td>0.035672</td>\n      <td>0.345505</td>\n      <td>0.051996</td>\n      <td>0.081450</td>\n      <td>0.009500</td>\n      <td>0.575735</td>\n      <td>0.445435</td>\n      <td>0.040300</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>334</th>\n      <td>0.221643</td>\n      <td>0.137322</td>\n      <td>0.415535</td>\n      <td>0.287241</td>\n      <td>0.174811</td>\n      <td>0.083198</td>\n      <td>0.832583</td>\n      <td>0.294103</td>\n      <td>0.264600</td>\n      <td>0.216215</td>\n      <td>0.492516</td>\n      <td>0.315594</td>\n      <td>0.291219</td>\n      <td>0.578705</td>\n      <td>0.350224</td>\n      <td>0.350926</td>\n      <td>0.506324</td>\n      <td>0.148171</td>\n      <td>0.046526</td>\n      <td>0.056868</td>\n    </tr>\n    <tr>\n      <th>335</th>\n      <td>1.014708</td>\n      <td>0.546206</td>\n      <td>0.264109</td>\n      <td>0.030659</td>\n      <td>0.210615</td>\n      <td>0.074433</td>\n      <td>0.013914</td>\n      <td>0.897492</td>\n      <td>0.106254</td>\n      <td>0.381597</td>\n      <td>0.503782</td>\n      <td>0.205373</td>\n      <td>0.082455</td>\n      <td>0.004882</td>\n      <td>0.329083</td>\n      <td>1.168812</td>\n      <td>0.055876</td>\n      <td>0.174809</td>\n      <td>0.199637</td>\n      <td>0.125589</td>\n    </tr>\n    <tr>\n      <th>336</th>\n      <td>0.444172</td>\n      <td>0.271113</td>\n      <td>0.015033</td>\n      <td>0.111857</td>\n      <td>0.426393</td>\n      <td>0.033244</td>\n      <td>0.360036</td>\n      <td>0.916466</td>\n      <td>0.668931</td>\n      <td>0.152731</td>\n      <td>0.860954</td>\n      <td>0.209770</td>\n      <td>0.148713</td>\n      <td>0.292803</td>\n      <td>0.420948</td>\n      <td>0.427907</td>\n      <td>0.056493</td>\n      <td>0.138351</td>\n      <td>0.597641</td>\n      <td>0.247192</td>\n    </tr>\n    <tr>\n      <th>337</th>\n      <td>0.107421</td>\n      <td>0.371932</td>\n      <td>0.629629</td>\n      <td>0.589626</td>\n      <td>0.270849</td>\n      <td>0.044645</td>\n      <td>0.225328</td>\n      <td>0.062849</td>\n      <td>0.192133</td>\n      <td>0.298835</td>\n      <td>0.769998</td>\n      <td>0.572180</td>\n      <td>0.596455</td>\n      <td>0.274310</td>\n      <td>0.671780</td>\n      <td>0.624567</td>\n      <td>0.053178</td>\n      <td>0.225962</td>\n      <td>0.735758</td>\n      <td>0.392469</td>\n    </tr>\n    <tr>\n      <th>338</th>\n      <td>0.856475</td>\n      <td>0.288589</td>\n      <td>0.289064</td>\n      <td>0.103272</td>\n      <td>0.136431</td>\n      <td>0.149399</td>\n      <td>0.090540</td>\n      <td>0.286858</td>\n      <td>0.723929</td>\n      <td>0.211244</td>\n      <td>0.654191</td>\n      <td>0.526241</td>\n      <td>0.205893</td>\n      <td>0.147172</td>\n      <td>0.151404</td>\n      <td>0.429274</td>\n      <td>0.305417</td>\n      <td>0.026174</td>\n      <td>0.054776</td>\n      <td>1.041088</td>\n    </tr>\n  </tbody>\n</table>\n<p>339 rows × 20 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "np.abs(fitted.predict(train_features) - train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}