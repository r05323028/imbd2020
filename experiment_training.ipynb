{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595223646977",
   "display_name": "Python 3.6.10 64-bit ('imbd2020': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from imbd.trainers import ModelTrainer\n",
    "from imbd.data import DataLoader\n",
    "from imbd.preprocessors import DataPreprocessor\n",
    "from imbd.models import KerasModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader()\n",
    "prepro = DataPreprocessor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    param_grid = {\n",
    "        \"prepro__variance_selector__threshold\": [0.0, 0.01],\n",
    "        \"model__epochs\": [10, 20],\n",
    "        \"model__dropout_rate\": [0.3, 0.1],\n",
    "        # \"model__estimator__n_estimators\": [1000],\n",
    "        # \"model__estimator__max_depth\": [5, 10],\n",
    "        # \"model__estimator__alpha\": [0, 0.1, 0.01],\n",
    "        # \"model__estimator__lambda\": [1, 0.5, 0.1],\n",
    "        # \"model__estimator__subsample\": [1, 0.5],\n",
    "        # \"model__estimator__gamma\": [0, 2, 10],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "loader = DataLoader()\n",
    "preprocessor = DataPreprocessor()\n",
    "df = loader.build()\n",
    "\n",
    "# get feature & label\n",
    "train_features = df.drop(loader.labels, axis=1)\n",
    "train_labels = df[loader.labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MultiOutputRegressor(XGBRegressor())\n",
    "# base_nn_model = KerasRegressor(build_fn=create_model, epochs=100)\n",
    "# base_nn_model = KerasRegressor(build_fn=classifier, epochs=100)\n",
    "base_nn_model = KerasModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "steps = [('prepro', preprocessor), ('model', base_nn_model)]\n",
    "pipe = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'memory': None,\n 'steps': [('prepro', DataPreprocessor()),\n  ('model', <imbd.models.KerasModel at 0x1423b2780>)],\n 'verbose': False,\n 'prepro': DataPreprocessor(),\n 'model': <imbd.models.KerasModel at 0x1423b2780>,\n 'prepro__drop_na_by_threshold': NADropper(),\n 'prepro__quantization': <imbd.preprocessors.QuantizationTransformer at 0x142343f28>,\n 'prepro__fill_na': <imbd.preprocessors.FillNATransformer at 0x142343ef0>,\n 'prepro__variance_selector': VarianceFeatureSelector(),\n 'prepro__outlier_detection': <imbd.preprocessors.OutlierDetector at 0x142343e80>,\n 'prepro__drop_na_by_threshold__na_threshold': 10,\n 'prepro__variance_selector__threshold': 0.0,\n 'model__build_fn': <function imbd.models.KerasModel.create_func_model(n_features, dropout_rate=0.3)>}"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "s 2ms/step - loss: 0.0287\nEpoch 70/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0289\nEpoch 71/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0287\nEpoch 72/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0289\nEpoch 73/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0289\nEpoch 74/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0288\nEpoch 75/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0284\nEpoch 76/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0286\nEpoch 77/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0290\nEpoch 78/100\n8/8 [==============================] - 0s 5ms/step - loss: 0.0286\nEpoch 79/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0288\nEpoch 80/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0286\nEpoch 81/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0285\nEpoch 82/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0283\nEpoch 83/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0282\nEpoch 84/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0287\nEpoch 85/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0287\nEpoch 86/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0287\nEpoch 87/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0287\nEpoch 88/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0288\nEpoch 89/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0290\nEpoch 90/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0285\nEpoch 91/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0284\nEpoch 92/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0287\nEpoch 93/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0289\nEpoch 94/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0285\nEpoch 95/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0289\nEpoch 96/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0288\nEpoch 97/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0286\nEpoch 98/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0287\nEpoch 99/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0287\nEpoch 100/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0289\n[CV]  model__dropout_rate=0.1, model__epochs=100, prepro__variance_selector__threshold=0.01, total=   4.9s\n[CV] model__dropout_rate=0.1, model__epochs=100, prepro__variance_selector__threshold=0.01 \n{'n_features': 111, 'dropout_rate': 0.1, 'epochs': 100, 'build_fn': <function KerasModel.create_func_model at 0x142384d08>}\nEpoch 1/100\n8/8 [==============================] - 0s 2ms/step - loss: 9.6394\nEpoch 2/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.1076\nEpoch 3/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0459\nEpoch 4/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0383\nEpoch 5/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0381\nEpoch 6/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0378\nEpoch 7/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0376\nEpoch 8/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0374\nEpoch 9/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0372\nEpoch 10/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0370\nEpoch 11/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0369\nEpoch 12/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0365\nEpoch 13/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0379\nEpoch 14/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0361\nEpoch 15/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0360\nEpoch 16/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0357\nEpoch 17/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0355\nEpoch 18/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0353\nEpoch 19/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0351\nEpoch 20/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0349\nEpoch 21/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0346\nEpoch 22/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0343\nEpoch 23/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0378\nEpoch 24/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0337\nEpoch 25/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0334\nEpoch 26/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0332\nEpoch 27/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0330\nEpoch 28/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0328\nEpoch 29/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0325\nEpoch 30/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0323\nEpoch 31/100\n8/8 [==============================] - 0s 5ms/step - loss: 0.0321\nEpoch 32/100\n8/8 [==============================] - 0s 5ms/step - loss: 0.0318\nEpoch 33/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0314\nEpoch 34/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0313\nEpoch 35/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0308\nEpoch 36/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0305\nEpoch 37/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0306\nEpoch 38/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0304\nEpoch 39/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0298\nEpoch 40/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0297\nEpoch 41/100\n8/8 [==============================] - 0s 5ms/step - loss: 0.0295\nEpoch 42/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0296\nEpoch 43/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0293\nEpoch 44/100\n8/8 [==============================] - 0s 5ms/step - loss: 0.0294\nEpoch 45/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0288\nEpoch 46/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0289\nEpoch 47/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0292\nEpoch 48/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0305\nEpoch 49/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0285\nEpoch 50/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0286\nEpoch 51/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0283\nEpoch 52/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0282\nEpoch 53/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0284\nEpoch 54/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0280\nEpoch 55/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0283\nEpoch 56/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0280\nEpoch 57/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0283\nEpoch 58/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0281\nEpoch 59/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0284\nEpoch 60/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0279\nEpoch 61/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0278\nEpoch 62/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0278\nEpoch 63/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0279\nEpoch 64/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0280\nEpoch 65/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0281\nEpoch 66/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0279\nEpoch 67/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0281\nEpoch 68/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0280\nEpoch 69/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0280\nEpoch 70/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0277\nEpoch 71/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0280\nEpoch 72/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0283\nEpoch 73/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0281\nEpoch 74/100\n8/8 [==============================] - 0s 8ms/step - loss: 0.0278\nEpoch 75/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0281\nEpoch 76/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0278\nEpoch 77/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0278\nEpoch 78/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0282\nEpoch 79/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0277\nEpoch 80/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0282\nEpoch 81/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0280\nEpoch 82/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0281\nEpoch 83/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0279\nEpoch 84/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0279\nEpoch 85/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0282\nEpoch 86/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0279\nEpoch 87/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0278\nEpoch 88/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0280\nEpoch 89/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0285\nEpoch 90/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0292\nEpoch 91/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0277\nEpoch 92/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0279\nEpoch 93/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0280\nEpoch 94/100\n8/8 [==============================] - 0s 5ms/step - loss: 0.0279\nEpoch 95/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0276\nEpoch 96/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0277\nEpoch 97/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0277\nEpoch 98/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0282\nEpoch 99/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0279\nEpoch 100/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0279\n[CV]  model__dropout_rate=0.1, model__epochs=100, prepro__variance_selector__threshold=0.01, total=   5.0s\n[CV] model__dropout_rate=0.1, model__epochs=100, prepro__variance_selector__threshold=0.01 \n{'n_features': 109, 'dropout_rate': 0.1, 'epochs': 100, 'build_fn': <function KerasModel.create_func_model at 0x142384d08>}\nEpoch 1/100\n8/8 [==============================] - 0s 2ms/step - loss: 117.0025\nEpoch 2/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.2767\nEpoch 3/100\n8/8 [==============================] - 0s 7ms/step - loss: 0.1102\nEpoch 4/100\n8/8 [==============================] - 0s 5ms/step - loss: 0.0912\nEpoch 5/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0570\nEpoch 6/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0565\nEpoch 7/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0558\nEpoch 8/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0553\nEpoch 9/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0549\nEpoch 10/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0541\nEpoch 11/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0534\nEpoch 12/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0526\nEpoch 13/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0521\nEpoch 14/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0511\nEpoch 15/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0502\nEpoch 16/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0499\nEpoch 17/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0486\nEpoch 18/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0484\nEpoch 19/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0474\nEpoch 20/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0468\nEpoch 21/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0454\nEpoch 22/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0453\nEpoch 23/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0448\nEpoch 24/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0663\nEpoch 25/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0436\nEpoch 26/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0437\nEpoch 27/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0432\nEpoch 28/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0440\nEpoch 29/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0423\nEpoch 30/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0434\nEpoch 31/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0424\nEpoch 32/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0425\nEpoch 33/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0415\nEpoch 34/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0439\nEpoch 35/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0417\nEpoch 36/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0412\nEpoch 37/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0413\nEpoch 38/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0423\nEpoch 39/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0410\nEpoch 40/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0410\nEpoch 41/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0416\nEpoch 42/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0416\nEpoch 43/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0422\nEpoch 44/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0574\nEpoch 45/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0415\nEpoch 46/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0425\nEpoch 47/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0410\nEpoch 48/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0414\nEpoch 49/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0424\nEpoch 50/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0411\nEpoch 51/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0419\nEpoch 52/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0415\nEpoch 53/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0429\nEpoch 54/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0420\nEpoch 55/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0417\nEpoch 56/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0415\nEpoch 57/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0413\nEpoch 58/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0417\nEpoch 59/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0417\nEpoch 60/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0421\nEpoch 61/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0415\nEpoch 62/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0415\nEpoch 63/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0455\nEpoch 64/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0424\nEpoch 65/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0407\nEpoch 66/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0418\nEpoch 67/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0428\nEpoch 68/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0410\nEpoch 69/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0407\nEpoch 70/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0415\nEpoch 71/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0412\nEpoch 72/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0405\nEpoch 73/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0426\nEpoch 74/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0408\nEpoch 75/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0401\nEpoch 76/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0415\nEpoch 77/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0417\nEpoch 78/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0418\nEpoch 79/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0410\nEpoch 80/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0406\nEpoch 81/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0413\nEpoch 82/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0410\nEpoch 83/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0415\nEpoch 84/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0413\nEpoch 85/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0418\nEpoch 86/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0407\nEpoch 87/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0421\nEpoch 88/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0405\nEpoch 89/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0420\nEpoch 90/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0417\nEpoch 91/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0415\nEpoch 92/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0414\nEpoch 93/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0420\nEpoch 94/100\n8/8 [==============================] - 0s 9ms/step - loss: 0.0414\nEpoch 95/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0421\nEpoch 96/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0414\nEpoch 97/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0412\nEpoch 98/100\n8/8 [==============================] - 0s 4ms/step - loss: 0.0412\nEpoch 99/100\n8/8 [==============================] - 0s 2ms/step - loss: 0.0405\nEpoch 100/100\n8/8 [==============================] - 0s 3ms/step - loss: 0.0412\n[CV]  model__dropout_rate=0.1, model__epochs=100, prepro__variance_selector__threshold=0.01, total=   4.9s\n[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:  1.4min finished\n{'n_features': 293, 'dropout_rate': 0.3, 'epochs': 10, 'build_fn': <function KerasModel.create_func_model at 0x142384d08>}\nEpoch 1/10\n11/11 [==============================] - 0s 10ms/step - loss: 2.9187\nEpoch 2/10\n11/11 [==============================] - 0s 9ms/step - loss: 0.0487\nEpoch 3/10\n11/11 [==============================] - 0s 3ms/step - loss: 0.0450\nEpoch 4/10\n11/11 [==============================] - 0s 3ms/step - loss: 0.0512\nEpoch 5/10\n11/11 [==============================] - 0s 3ms/step - loss: 0.0431\nEpoch 6/10\n11/11 [==============================] - 0s 3ms/step - loss: 0.0425\nEpoch 7/10\n11/11 [==============================] - 0s 4ms/step - loss: 0.0494\nEpoch 8/10\n11/11 [==============================] - 0s 3ms/step - loss: 0.0413\nEpoch 9/10\n11/11 [==============================] - 0s 3ms/step - loss: 0.0404\nEpoch 10/10\n11/11 [==============================] - 0s 2ms/step - loss: 0.0397\n"
    }
   ],
   "source": [
    "# training\n",
    "trainer = ModelTrainer(pipe=pipe, param_grid=param_grid, verbose=2)\n",
    "fitted = trainer.train(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.20621413610369368"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "fitted.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0       1.355651      0.284143         0.287637        0.051699   \n1       1.060265      0.033802         0.270902        0.020839   \n2       4.050157      0.834316         0.547688        0.133316   \n3       4.467174      0.322863         0.658920        0.086671   \n4       2.100579      0.316068         0.647453        0.208571   \n5       1.347657      0.013287         0.448769        0.024791   \n6       4.507659      0.209913         0.822383        0.246700   \n7       4.256617      0.083006         0.679731        0.029866   \n\n  param_model__dropout_rate param_model__epochs  \\\n0                       0.3                  10   \n1                       0.3                  10   \n2                       0.3                 100   \n3                       0.3                 100   \n4                       0.1                  10   \n5                       0.1                  10   \n6                       0.1                 100   \n7                       0.1                 100   \n\n  param_prepro__variance_selector__threshold  \\\n0                                          0   \n1                                       0.01   \n2                                          0   \n3                                       0.01   \n4                                          0   \n5                                       0.01   \n6                                          0   \n7                                       0.01   \n\n                                              params  split0_test_score  \\\n0  {'model__dropout_rate': 0.3, 'model__epochs': ...           0.227192   \n1  {'model__dropout_rate': 0.3, 'model__epochs': ...           0.232526   \n2  {'model__dropout_rate': 0.3, 'model__epochs': ...           0.201107   \n3  {'model__dropout_rate': 0.3, 'model__epochs': ...           0.201118   \n4  {'model__dropout_rate': 0.1, 'model__epochs': ...           0.223066   \n5  {'model__dropout_rate': 0.1, 'model__epochs': ...           0.228172   \n6  {'model__dropout_rate': 0.1, 'model__epochs': ...           0.200676   \n7  {'model__dropout_rate': 0.1, 'model__epochs': ...           0.200204   \n\n   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n0           0.236089           0.152075         0.205119        0.037683   \n1           0.234802           0.151314         0.206214        0.038831   \n2           0.206302           0.123238         0.176882        0.037992   \n3           0.207784           0.123434         0.177445        0.038289   \n4           0.227190           0.147842         0.199366        0.036472   \n5           0.240943           0.144192         0.204436        0.042916   \n6           0.205851           0.124162         0.176896        0.037349   \n7           0.205056           0.125332         0.176864        0.036493   \n\n   rank_test_score  \n0                2  \n1                1  \n2                7  \n3                5  \n4                4  \n5                3  \n6                6  \n7                8  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_model__dropout_rate</th>\n      <th>param_model__epochs</th>\n      <th>param_prepro__variance_selector__threshold</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.355651</td>\n      <td>0.284143</td>\n      <td>0.287637</td>\n      <td>0.051699</td>\n      <td>0.3</td>\n      <td>10</td>\n      <td>0</td>\n      <td>{'model__dropout_rate': 0.3, 'model__epochs': ...</td>\n      <td>0.227192</td>\n      <td>0.236089</td>\n      <td>0.152075</td>\n      <td>0.205119</td>\n      <td>0.037683</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.060265</td>\n      <td>0.033802</td>\n      <td>0.270902</td>\n      <td>0.020839</td>\n      <td>0.3</td>\n      <td>10</td>\n      <td>0.01</td>\n      <td>{'model__dropout_rate': 0.3, 'model__epochs': ...</td>\n      <td>0.232526</td>\n      <td>0.234802</td>\n      <td>0.151314</td>\n      <td>0.206214</td>\n      <td>0.038831</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.050157</td>\n      <td>0.834316</td>\n      <td>0.547688</td>\n      <td>0.133316</td>\n      <td>0.3</td>\n      <td>100</td>\n      <td>0</td>\n      <td>{'model__dropout_rate': 0.3, 'model__epochs': ...</td>\n      <td>0.201107</td>\n      <td>0.206302</td>\n      <td>0.123238</td>\n      <td>0.176882</td>\n      <td>0.037992</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.467174</td>\n      <td>0.322863</td>\n      <td>0.658920</td>\n      <td>0.086671</td>\n      <td>0.3</td>\n      <td>100</td>\n      <td>0.01</td>\n      <td>{'model__dropout_rate': 0.3, 'model__epochs': ...</td>\n      <td>0.201118</td>\n      <td>0.207784</td>\n      <td>0.123434</td>\n      <td>0.177445</td>\n      <td>0.038289</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.100579</td>\n      <td>0.316068</td>\n      <td>0.647453</td>\n      <td>0.208571</td>\n      <td>0.1</td>\n      <td>10</td>\n      <td>0</td>\n      <td>{'model__dropout_rate': 0.1, 'model__epochs': ...</td>\n      <td>0.223066</td>\n      <td>0.227190</td>\n      <td>0.147842</td>\n      <td>0.199366</td>\n      <td>0.036472</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1.347657</td>\n      <td>0.013287</td>\n      <td>0.448769</td>\n      <td>0.024791</td>\n      <td>0.1</td>\n      <td>10</td>\n      <td>0.01</td>\n      <td>{'model__dropout_rate': 0.1, 'model__epochs': ...</td>\n      <td>0.228172</td>\n      <td>0.240943</td>\n      <td>0.144192</td>\n      <td>0.204436</td>\n      <td>0.042916</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4.507659</td>\n      <td>0.209913</td>\n      <td>0.822383</td>\n      <td>0.246700</td>\n      <td>0.1</td>\n      <td>100</td>\n      <td>0</td>\n      <td>{'model__dropout_rate': 0.1, 'model__epochs': ...</td>\n      <td>0.200676</td>\n      <td>0.205851</td>\n      <td>0.124162</td>\n      <td>0.176896</td>\n      <td>0.037349</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>4.256617</td>\n      <td>0.083006</td>\n      <td>0.679731</td>\n      <td>0.029866</td>\n      <td>0.1</td>\n      <td>100</td>\n      <td>0.01</td>\n      <td>{'model__dropout_rate': 0.1, 'model__epochs': ...</td>\n      <td>0.200204</td>\n      <td>0.205056</td>\n      <td>0.125332</td>\n      <td>0.176864</td>\n      <td>0.036493</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "pd.DataFrame(fitted.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     Input_A6_024  Input_A3_016  Input_C_013  Input_A2_016  Input_A3_017  \\\n0        0.004014      0.043178     0.003052      0.010233       0.01365   \n1        0.004014      0.033178     0.002052      0.000233       0.00365   \n2        0.015986      0.063178     0.003052      0.019767       0.04365   \n3        0.015986      0.063178     0.002052      0.019767       0.00365   \n4        0.004014      0.063178     0.003052      0.009767       0.01365   \n..            ...           ...          ...           ...           ...   \n334      0.004014      0.033178     0.003052      0.020233       0.01365   \n335      0.015986      0.023178     0.003052      0.010233       0.01365   \n336      0.035986      0.033178     0.003052      0.010233       0.02365   \n337      0.015986      0.033178     0.003052      0.020233       0.00365   \n338      0.015986      0.033178     0.003052      0.010233       0.01365   \n\n     Input_C_050  Input_A6_001  Input_C_096  Input_A3_018  Input_A6_019  \\\n0       0.049748       0.14488     0.012559      0.012205      0.013137   \n1       0.049748       0.01512     0.032559      0.002205      0.003137   \n2       0.049748       0.01512     0.012559      0.022205      0.026863   \n3       0.051748       0.01512     0.032559      0.017795      0.013137   \n4       0.051748       0.01512     0.022559      0.012205      0.003137   \n..           ...           ...          ...           ...           ...   \n334     0.046048       0.01512     0.022559      0.017795      0.003137   \n335     0.045948       0.01512     0.022559      0.017795      0.003137   \n336     0.045748       0.01512     0.022559      0.007795      0.003137   \n337     0.046148       0.01512     0.022559      0.002205      0.013137   \n338     0.046248       0.01512     0.032559      0.002205      0.006863   \n\n     Input_A1_020  Input_A6_011  Input_A3_015  Input_C_046  Input_C_049  \\\n0        0.059031      0.019009      0.021889     0.025638     0.001047   \n1        0.040969      0.021009      0.038111     0.025638     0.000947   \n2        0.059031      0.020009      0.011889     0.025438     0.000847   \n3        0.840969      0.019009      0.041889     0.025138     0.001347   \n4        0.159031      0.020009      0.001889     0.025538     0.000847   \n..            ...           ...           ...          ...          ...   \n334      0.059031      0.023009      0.038111     0.025738     0.000847   \n335      0.059031      0.025009      0.038111     0.026038     0.000447   \n336      0.059031      0.025009      0.028111     0.025138     0.000747   \n337      0.040969      0.025009      0.038111     0.025838     0.000547   \n338      0.340969      0.023009      0.028111     0.025738     0.000647   \n\n     Input_A2_024  Input_C_058  Input_C_057  Input_A3_013  Input_A2_017  \n0        0.004156     0.007833     0.008508      0.020316      0.030284  \n1        0.004156     0.006833     0.014508      0.020316      0.020284  \n2        0.024156     0.002833     0.015508      0.020316      0.009716  \n3        0.034156     0.002167     0.013508      0.020316      0.020284  \n4        0.024156     0.006833     0.017508      0.022316      0.000284  \n..            ...          ...          ...           ...           ...  \n334      0.024156     0.011333     0.000508      0.020316      0.020284  \n335      0.034156     0.011533     0.000808      0.020316      0.020284  \n336      0.024156     0.011133     0.000092      0.022316      0.030284  \n337      0.034156     0.012033     0.000608      0.020316      0.020284  \n338      0.024156     0.010933     0.000508      0.018316      0.010284  \n\n[339 rows x 20 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Input_A6_024</th>\n      <th>Input_A3_016</th>\n      <th>Input_C_013</th>\n      <th>Input_A2_016</th>\n      <th>Input_A3_017</th>\n      <th>Input_C_050</th>\n      <th>Input_A6_001</th>\n      <th>Input_C_096</th>\n      <th>Input_A3_018</th>\n      <th>Input_A6_019</th>\n      <th>Input_A1_020</th>\n      <th>Input_A6_011</th>\n      <th>Input_A3_015</th>\n      <th>Input_C_046</th>\n      <th>Input_C_049</th>\n      <th>Input_A2_024</th>\n      <th>Input_C_058</th>\n      <th>Input_C_057</th>\n      <th>Input_A3_013</th>\n      <th>Input_A2_017</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.004014</td>\n      <td>0.043178</td>\n      <td>0.003052</td>\n      <td>0.010233</td>\n      <td>0.01365</td>\n      <td>0.049748</td>\n      <td>0.14488</td>\n      <td>0.012559</td>\n      <td>0.012205</td>\n      <td>0.013137</td>\n      <td>0.059031</td>\n      <td>0.019009</td>\n      <td>0.021889</td>\n      <td>0.025638</td>\n      <td>0.001047</td>\n      <td>0.004156</td>\n      <td>0.007833</td>\n      <td>0.008508</td>\n      <td>0.020316</td>\n      <td>0.030284</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.004014</td>\n      <td>0.033178</td>\n      <td>0.002052</td>\n      <td>0.000233</td>\n      <td>0.00365</td>\n      <td>0.049748</td>\n      <td>0.01512</td>\n      <td>0.032559</td>\n      <td>0.002205</td>\n      <td>0.003137</td>\n      <td>0.040969</td>\n      <td>0.021009</td>\n      <td>0.038111</td>\n      <td>0.025638</td>\n      <td>0.000947</td>\n      <td>0.004156</td>\n      <td>0.006833</td>\n      <td>0.014508</td>\n      <td>0.020316</td>\n      <td>0.020284</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.015986</td>\n      <td>0.063178</td>\n      <td>0.003052</td>\n      <td>0.019767</td>\n      <td>0.04365</td>\n      <td>0.049748</td>\n      <td>0.01512</td>\n      <td>0.012559</td>\n      <td>0.022205</td>\n      <td>0.026863</td>\n      <td>0.059031</td>\n      <td>0.020009</td>\n      <td>0.011889</td>\n      <td>0.025438</td>\n      <td>0.000847</td>\n      <td>0.024156</td>\n      <td>0.002833</td>\n      <td>0.015508</td>\n      <td>0.020316</td>\n      <td>0.009716</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.015986</td>\n      <td>0.063178</td>\n      <td>0.002052</td>\n      <td>0.019767</td>\n      <td>0.00365</td>\n      <td>0.051748</td>\n      <td>0.01512</td>\n      <td>0.032559</td>\n      <td>0.017795</td>\n      <td>0.013137</td>\n      <td>0.840969</td>\n      <td>0.019009</td>\n      <td>0.041889</td>\n      <td>0.025138</td>\n      <td>0.001347</td>\n      <td>0.034156</td>\n      <td>0.002167</td>\n      <td>0.013508</td>\n      <td>0.020316</td>\n      <td>0.020284</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.004014</td>\n      <td>0.063178</td>\n      <td>0.003052</td>\n      <td>0.009767</td>\n      <td>0.01365</td>\n      <td>0.051748</td>\n      <td>0.01512</td>\n      <td>0.022559</td>\n      <td>0.012205</td>\n      <td>0.003137</td>\n      <td>0.159031</td>\n      <td>0.020009</td>\n      <td>0.001889</td>\n      <td>0.025538</td>\n      <td>0.000847</td>\n      <td>0.024156</td>\n      <td>0.006833</td>\n      <td>0.017508</td>\n      <td>0.022316</td>\n      <td>0.000284</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>334</th>\n      <td>0.004014</td>\n      <td>0.033178</td>\n      <td>0.003052</td>\n      <td>0.020233</td>\n      <td>0.01365</td>\n      <td>0.046048</td>\n      <td>0.01512</td>\n      <td>0.022559</td>\n      <td>0.017795</td>\n      <td>0.003137</td>\n      <td>0.059031</td>\n      <td>0.023009</td>\n      <td>0.038111</td>\n      <td>0.025738</td>\n      <td>0.000847</td>\n      <td>0.024156</td>\n      <td>0.011333</td>\n      <td>0.000508</td>\n      <td>0.020316</td>\n      <td>0.020284</td>\n    </tr>\n    <tr>\n      <th>335</th>\n      <td>0.015986</td>\n      <td>0.023178</td>\n      <td>0.003052</td>\n      <td>0.010233</td>\n      <td>0.01365</td>\n      <td>0.045948</td>\n      <td>0.01512</td>\n      <td>0.022559</td>\n      <td>0.017795</td>\n      <td>0.003137</td>\n      <td>0.059031</td>\n      <td>0.025009</td>\n      <td>0.038111</td>\n      <td>0.026038</td>\n      <td>0.000447</td>\n      <td>0.034156</td>\n      <td>0.011533</td>\n      <td>0.000808</td>\n      <td>0.020316</td>\n      <td>0.020284</td>\n    </tr>\n    <tr>\n      <th>336</th>\n      <td>0.035986</td>\n      <td>0.033178</td>\n      <td>0.003052</td>\n      <td>0.010233</td>\n      <td>0.02365</td>\n      <td>0.045748</td>\n      <td>0.01512</td>\n      <td>0.022559</td>\n      <td>0.007795</td>\n      <td>0.003137</td>\n      <td>0.059031</td>\n      <td>0.025009</td>\n      <td>0.028111</td>\n      <td>0.025138</td>\n      <td>0.000747</td>\n      <td>0.024156</td>\n      <td>0.011133</td>\n      <td>0.000092</td>\n      <td>0.022316</td>\n      <td>0.030284</td>\n    </tr>\n    <tr>\n      <th>337</th>\n      <td>0.015986</td>\n      <td>0.033178</td>\n      <td>0.003052</td>\n      <td>0.020233</td>\n      <td>0.00365</td>\n      <td>0.046148</td>\n      <td>0.01512</td>\n      <td>0.022559</td>\n      <td>0.002205</td>\n      <td>0.013137</td>\n      <td>0.040969</td>\n      <td>0.025009</td>\n      <td>0.038111</td>\n      <td>0.025838</td>\n      <td>0.000547</td>\n      <td>0.034156</td>\n      <td>0.012033</td>\n      <td>0.000608</td>\n      <td>0.020316</td>\n      <td>0.020284</td>\n    </tr>\n    <tr>\n      <th>338</th>\n      <td>0.015986</td>\n      <td>0.033178</td>\n      <td>0.003052</td>\n      <td>0.010233</td>\n      <td>0.01365</td>\n      <td>0.046248</td>\n      <td>0.01512</td>\n      <td>0.032559</td>\n      <td>0.002205</td>\n      <td>0.006863</td>\n      <td>0.340969</td>\n      <td>0.023009</td>\n      <td>0.028111</td>\n      <td>0.025738</td>\n      <td>0.000647</td>\n      <td>0.024156</td>\n      <td>0.010933</td>\n      <td>0.000508</td>\n      <td>0.018316</td>\n      <td>0.010284</td>\n    </tr>\n  </tbody>\n</table>\n<p>339 rows × 20 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "np.abs(fitted.predict(train_features) - train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'build_fn': <function imbd.models.KerasModel.create_func_model(n_features, dropout_rate=0.3)>}"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "pipe.get_params()['model'].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}